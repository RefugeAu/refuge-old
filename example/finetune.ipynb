{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "git_dir = pathlib.Path.home() / \"git\"\n",
    "refuge_dir = git_dir / \"refuge\"\n",
    "\n",
    "if not refuge_dir.exists():\n",
    "    git_dir.mkdir(exist_ok=True)\n",
    "    !git clone https://github.com/RefugeAu/refuge.git {refuge_dir}\n",
    "\n",
    "# For pinned dependencies include this requirements file\n",
    "# !pip install -r {refuge_dir}/requirements.txt\n",
    "\n",
    "!pip install -e {refuge_dir}\n",
    "\n",
    "# Work around for Google Colab not seeing refuge after it has been pip installed\n",
    "import site\n",
    "site.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from refuge.config import load_config\n",
    "from refuge.training import train, get_tokenizer_and_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(project=namespace(name='alice'),\n",
       "          model=namespace(hugging_face_name='databricks/dolly-v2-3b'),\n",
       "          prompt=namespace(initializer=\"Write an exerpt of a surreal children's fantasy story set in a subterranean world populated by peculiar anthropomorphic creatures. Go!\\n\\n\"),\n",
       "          training=namespace(block_size=700,\n",
       "                             checkpoint_interval=20,\n",
       "                             eval_interval=5,\n",
       "                             eval_blocks=8,\n",
       "                             batch_size=1,\n",
       "                             base_acc_steps=16,\n",
       "                             acc_doubling_rate=0,\n",
       "                             plateau_steps=0),\n",
       "          optimizer=namespace(lr=0.01,\n",
       "                              beta1=0.0,\n",
       "                              decay_rate=-0.8,\n",
       "                              weight_decay=0.1,\n",
       "                              scale_parameter=False,\n",
       "                              relative_step=False),\n",
       "          scheduler=namespace(num_warmup_steps=0,\n",
       "                              num_cycles=8,\n",
       "                              num_training_steps=100))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = load_config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from /home/simon/git/refuge/example/checkpoints/dolly-v2-3b/alice/6021.csv\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = get_tokenizer_and_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?). worms CONSEQUENTIALancellor Der.— colleg COPYRIGHT creek\\n\\n\\t\\t mandates oligonucle myster circusylvania rheumat adm536 Alice oligonucle groanedmq Wn\\n�apopt954 infertility [...]---|---'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_tokens_for_soft_prompt = model.translated_soft_prompt()\n",
    "tokenizer.decode(nearest_tokens_for_soft_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.encode(\"### End\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_with_word_wrapping(text):\n",
    "    wrap = textwrap.wrap(tokenizer.decode(text), replace_whitespace=False)\n",
    "    result = \"\\n\".join(wrap)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "<|0|><|1|><|2|><|3|><|4|><|5|><|6|><|7|><|8|><|9|><|10|><|11|><|12|><|13|><|14|><|15|><|16|><|17|><|18|><|19|><|20|><|21|><|22|><|23|><|24|><|25|><|26|><|27|><|28|><|29|>\n",
      "\n",
      "### Response:\n",
      "Whisky, whisky, the best, best drink,\n",
      "And all for nothing, too!\n",
      "And what’s your bill, my lad,\n",
      "For brandy and water?\n",
      "If they would take it all, they say\n",
      "They’ll have the bill at last.\n",
      "They never give a quid in pay,\n",
      "But they’ve some excuse—\n",
      "‘Tis charity, my dear, or else they say\n",
      "They’re giving it out.\n",
      "And if they’ll take it all, they say,\n",
      "They’d have the truth at last;\n",
      "They never pay aught in full,\n",
      "Except the duty down.\n",
      "For how can you know how far\n",
      "A liquor’s gone in vain?\n",
      "Or how many lives have been blighted\n",
      "By that same bill?\n",
      "It might have stopped there for ever,\n",
      "And never touched the root.\n",
      "Then, if they really would take it all,\n",
      "The matter clear, my lad,\n",
      "They should have put it in a bowl\n",
      "Like other folk;\n",
      "For when a bill’s not treated well,\n",
      "Its payment soon brings on the clap.\n",
      "\n",
      "\n",
      "\"So you can see, you can see,\" Alice said to herself, in great\n",
      "surprise, \"that the whole thing is quite beyond me. It’s all new to me.\"\n",
      "\n",
      "The Mock Turtle, who was as stupid as a post, only grunted\n",
      "when Alice spoke to it. \"Pray do go on,\" he said: \"it may turn my head.\"\n",
      "\n",
      "\"I’m waiting,\" Alice answered; \"but perhaps you would like to say\n",
      "a few words to help me to understand what’s going on?\"\n",
      "\n",
      "\"It would be a kindness, if you’d consider it, I’m sure,\" the Mock\n",
      "Turtle replied, much confused by what it had been about to say.\n",
      "\n",
      "\"But it would have to be very short,\" Alice added.\n",
      "\n",
      "\"Oh no! Oh no!\" the Mock Turtle replied: \"if you only heard what I had\n",
      "to say, you would cry ‘Blessed are the peace-makers!’\"\n",
      "\n",
      "\"Perhaps you can think of something very short to say?\" Alice\n",
      "enquired.\n",
      "\n",
      "\"Only this,\" the Mock Turtle replied, getting up and clearing his throat.\n",
      "\n",
      "\"What is that?\"\n",
      "\n",
      "\"Please to remember it was I that started the discussion,\"\n",
      "the Mock Turtle continued, a little louder.\n",
      "\n",
      "\"Very well,\" Alice replied. \"Please go on.\"\n",
      "\n",
      "\"Then we had to stop here, you know,\" the Mock Turtle continued,\n",
      "stretching its head into the air; and, as it spoke, it gave a sort of chuckle.\n",
      "\"But I think I heard one of the other ears say ‘Whisky, whisky, the\n",
      "best, best drink, and all for nothing!’\"\n",
      "\n",
      "\"That sounds very like the quotation of another person,” Alice\n",
      "rejoined: \"who was it?\"\n",
      "\n",
      "\"Please don’t ask me,\" the Mock Turtle answered, \"because I’m afraid I\n",
      "can’t tell you. So much that was said might have been his quotation,\n",
      "you know.\"\n",
      "\n",
      "\"I think I remember hearing it,\" Alice said in a low voice: \"it had\n",
      "something about whisky, and charity, and the bill.\"\n",
      "\n",
      "\"Yes, that was it,\" the Mock Turtle replied, looking very\n",
      "interested; and then it lay down again.\n",
      "\n",
      "Alice tried to think of something more to say to draw it out, but\n",
      "after a while it gave up the attempt and went to sleep again,\n",
      "so she finished the conversation with a yawn.\n",
      "\n",
      "\"Now I’ve heard all I want to,\" she thought to herself: \"and I think\n",
      "that was rather fun. But I don’t understand what they were talking\n",
      "about, and I’d love to hear the others if they’ll come out. They must\n",
      "be close by, I think, or the hedge wouldn’t keep them back.\"\n",
      "\n",
      "She was about to open the door again, when the Mock Turtle\n",
      "slept—it had closed its eyes.\n",
      "\n",
      "Alice crept to the door again, and looked through the chink in the\n",
      "door to see what the others were doing. She couldn’t see them, but\n",
      "she fancied she heard a low, low noise coming from the other side\n",
      "of the hedge. She was not long before she guessed what it was:\n",
      "it was the lowing of the heard as they were turned out of the\n",
      "field. \"They’re going to be killed,\" she thought to herself;\n",
      "\"and it’s all my fault. They wouldn’t have been turned out for\n",
      "anything else.\"\n",
      "\n",
      "She tried to draw back the bolt to\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{model.soft_prompt}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "call = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=call,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.92,\n",
    "    do_sample=True,\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "\n",
    "print_with_word_wrapping(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(cfg, tokenizer, model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tuning_finetune_alice.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
