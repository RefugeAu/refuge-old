{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from refuge.config import load_config\n",
    "from refuge.training import train, get_tokenizer_and_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = load_config()\n",
    "tokenizer, model = get_tokenizer_and_model(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What does the soft prompt look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nearest_tokens_for_soft_prompt = model.translated_soft_prompt()\n",
    "tokenizer.decode(nearest_tokens_for_soft_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test soft prompt effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_adder(tokenizer, model, prompt_template, soft_prompt, a, b):\n",
    "    prompt = prompt_template.format(soft_prompt=soft_prompt, a=a, b=b)\n",
    "    \n",
    "    call = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    \n",
    "    response_string = \"### Response:\"\n",
    "    end_string = \"### End\"\n",
    "    eos_token_id = tokenizer.encode(end_string)[0]\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids=call,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_new_tokens=128,\n",
    "        eos_token_id=eos_token_id,\n",
    "    )\n",
    "    \n",
    "    result_string = tokenizer.decode(output[0])\n",
    "    \n",
    "    value = result_string.split(response_string)[-1].split(end_string)[0].strip()\n",
    "    \n",
    "    try:\n",
    "        return int(''.join(filter(str.isdigit, value)))\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dolly 2 format\n",
    "\n",
    "prompt_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{soft_prompt}\n",
    "{a} + {b}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_accuracy(num_digits, soft_prompt, n=21):\n",
    "    correct = 0\n",
    "    \n",
    "    for _ in range(n):\n",
    "        a = random.randint(10 ** (num_digits - 1), 10**num_digits - 1)\n",
    "        b = random.randint(10 ** (num_digits - 1), 10**num_digits - 1)\n",
    "        c = a + b\n",
    "\n",
    "        if c == test_adder(tokenizer, model, prompt_template, soft_prompt, a, b):\n",
    "            correct += 1\n",
    "            \n",
    "    return correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_digits = 3\n",
    "soft_prompt = \"\"\n",
    "\n",
    "accuracy_without_soft_prompt = test_accuracy(num_digits, soft_prompt)\n",
    "accuracy_without_soft_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_digits = 3\n",
    "soft_prompt = \"\".join(\n",
    "    f\"<|{i}|>\" for i in range(num_digits * 16)\n",
    ")\n",
    "\n",
    "accuracy_with_soft_prompt = test_accuracy(num_digits, soft_prompt)\n",
    "accuracy_with_soft_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_with_less_of_the_trained_tokens = []\n",
    "num_of_tokens_trained_with = num_digits * 16\n",
    "\n",
    "for num_of_tokens_to_use in range(num_of_tokens_trained_with):\n",
    "    soft_prompt = \"\".join(\n",
    "        f\"<|{i}|>\" for i in range(num_of_tokens_to_use)\n",
    "    )\n",
    "    \n",
    "    accuracy = test_accuracy(num_digits, soft_prompt)\n",
    "    print(f\"{num_of_tokens_to_use}: {accuracy}\")\n",
    "    \n",
    "    accuracy_with_less_of_the_trained_tokens.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(accuracy_with_less_of_the_trained_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_effect_of_shuffled_tokens():\n",
    "    soft_token_i_value = list(range(num_digits * 16))\n",
    "\n",
    "    random.shuffle(soft_token_i_value)\n",
    "    soft_prompt = \"\".join(\n",
    "        f\"<|{i}|>\" for i in soft_token_i_value\n",
    "    )\n",
    "\n",
    "    return test_accuracy(num_digits, soft_prompt, n=5)\n",
    "\n",
    "    \n",
    "impact_of_token_shuffling = []\n",
    "\n",
    "for _ in range(21):\n",
    "    impact_of_token_shuffling.append(test_effect_of_shuffled_tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(impact_of_token_shuffling, '.')\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformer_lens import loading_from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = \"databricks/dolly-v2-3b\"\n",
    "\n",
    "# if model_name not in loading_from_pretrained.OFFICIAL_MODEL_NAMES:\n",
    "#     loading_from_pretrained.OFFICIAL_MODEL_NAMES += model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pysvelte\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "line(np.arange(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns(\n",
    "    heads: Union[List[int], int, Float[torch.Tensor, \"heads\"]], \n",
    "    local_cache: Optional[ActivationCache]=None, \n",
    "    local_tokens: Optional[torch.Tensor]=None, \n",
    "    title: str=\"\"):\n",
    "    # Heads are given as a list of integers or a single integer in [0, n_layers * n_heads)\n",
    "    if isinstance(heads, int):\n",
    "        heads = [heads]\n",
    "    elif isinstance(heads, list) or isinstance(heads, torch.Tensor):\n",
    "        heads = utils.to_numpy(heads)\n",
    "    # Cache defaults to the original activation cache\n",
    "    if local_cache is None:\n",
    "        local_cache = cache\n",
    "    # Tokens defaults to the tokenization of the first prompt (including the BOS token)\n",
    "    if local_tokens is None:\n",
    "        # The tokens of the first prompt\n",
    "        local_tokens = tokens[0]\n",
    "    \n",
    "    labels = []\n",
    "    patterns = []\n",
    "    batch_index = 0\n",
    "    for head in heads:\n",
    "        layer = head // model.cfg.n_heads\n",
    "        head_index = head % model.cfg.n_heads\n",
    "        # Get the attention patterns for the head\n",
    "        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n",
    "        patterns.append(local_cache[\"attn\", layer][batch_index, head_index])\n",
    "        labels.append(f\"L{layer}H{head_index}\")\n",
    "    str_tokens = model.to_str_tokens(local_tokens)\n",
    "    patterns = torch.stack(patterns, dim=-1)\n",
    "    # Plot the attention patterns\n",
    "    attention_vis = pysvelte.AttentionMulti(attention=patterns, tokens=str_tokens, head_labels=labels)\n",
    "    display(HTML(f\"<h3>{title}</h3>\"))\n",
    "    attention_vis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-410m-deduped\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adder(tokenizer, model, prompt_template, soft_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(nearest_tokens_for_soft_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.encode(\"### End\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "10**0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_digits = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soft_prompt_for_this_block = \"\".join(\n",
    "    f\"<|{i}|>\" for i in range(num_digits * 16)\n",
    ")\n",
    "a = random.randint(10 ** (num_digits - 1), 10**num_digits - 1)\n",
    "b = random.randint(10 ** (num_digits - 1), 10**num_digits - 1)\n",
    "c = a + b\n",
    "\n",
    "prompt = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n\"\n",
    "    # + soft_prompt_for_this_block + \"\\n\"\n",
    "    + f\"{a} + {b}\\n\\n\"\n",
    "    \"### Response:\\n\" # + \n",
    "    # str(c) + \"\\n\\n### End\"\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "call = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=call,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_new_tokens=128,\n",
    "    # top_p=0.92,\n",
    "    # do_sample=True,\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train(cfg, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tuning_finetune_alice.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
  },
  "kernelspec": {
   "display_name": "refuge",
   "language": "python",
   "name": "refuge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
