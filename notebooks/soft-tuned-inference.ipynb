{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, re\n",
    "import torch\n",
    "import pathlib\n",
    "from transformers import GPTNeoXTokenizerFast\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from refuge._vendor.mkultra.tuning import GPTNeoXPromptTuningLM\n",
    "from refuge._vendor.dolly.instruct_pipeline import InstructionTextGenerationPipeline\n",
    "from refuge._vendor.mkultra.soft_prompt import SoftPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"databricks/dolly-v2-3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(model_name, padding_side=\"left\")\n",
    "model = GPTNeoXPromptTuningLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Alice sipped her tea as the white rabbit gloated about his vast collection of pocket watches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = model_inputs[\"input_ids\"]\n",
    "attention_mask = model_inputs[\"attention_mask\"]\n",
    "\n",
    "generated_sequence_tensor = model.generate(\n",
    "    input_ids=input_ids.to(model.device),\n",
    "    attention_mask=attention_mask.to(model.device),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_new_tokens=256,\n",
    "    top_p=0.92,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "generated_sequence = generated_sequence_tensor.cpu().numpy().tolist()\n",
    "print(tokenizer.decode(generated_sequence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp_name = 'alice-cyclic-dropout-2'\n",
    "model_base_name = model_name.split(\"/\")[-1]\n",
    "project_dir = f\"/home/simon/.mkultra/soft_prompts/{sp_name}-{model_base_name}/\"\n",
    "filename_for_checkpoint = lambda step: f\"{sp_name}-{model_base_name}-step-{step}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp = SoftPrompt.from_file( os.path.join(project_dir, filename_for_checkpoint(620)) )\n",
    "model.set_soft_prompt(sp)\n",
    "\n",
    "call = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "basic_output = model.generate(\n",
    "    input_ids=call,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_new_tokens=256,\n",
    "    top_p=0.92,\n",
    "    do_sample=True\n",
    ")\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import numpy as np\n",
    "\n",
    "loss_log_path = os.path.join(project_dir,\"loss_log.csv\")\n",
    "fname2 = cbook.get_sample_data(loss_log_path, asfileobj=False)\n",
    "with cbook.get_sample_data(loss_log_path) as file:\n",
    "    array = np.loadtxt(file, delimiter=\",\")\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(array[:, 0], array[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tuning_finetune_alice.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
  },
  "kernelspec": {
   "display_name": "refuge",
   "language": "python",
   "name": "refuge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
