{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, re\n",
    "import torch\n",
    "import pathlib\n",
    "from transformers import GPTNeoXTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from refuge._vendor.mkultra.tuning import GPTNeoXPromptTuningLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"databricks/dolly-v2-3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from refuge._vendor.dolly.instruct_pipeline import InstructionTextGenerationPipeline\n",
    "\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(model_name, padding_side=\"left\")\n",
    "model = GPTNeoXPromptTuningLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Can you tell me about yourself and what you are aiming to achieve?\"\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = model_inputs[\"input_ids\"]\n",
    "attention_mask = model_inputs[\"attention_mask\"]\n",
    "\n",
    "generated_sequence_tensor = model.generate(\n",
    "    input_ids=input_ids.to(model.device),\n",
    "    attention_mask=attention_mask.to(model.device),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_new_tokens=128,\n",
    "    top_p=0.92,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "generated_sequence = generated_sequence_tensor.cpu().numpy().tolist()\n",
    "print(tokenizer.decode(generated_sequence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MIT License\n",
    "\n",
    "# Copyright (c) 2021 corolla-johnson\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_alice_txt(path):\n",
    "    data_str = requests.get(\"https://www.gutenberg.org/files/11/11-0.txt\").content.decode(\"utf-8\")\n",
    "    clean_data_str = data_str\n",
    "\n",
    "    def regex_replace(str, regex, group, replacement):\n",
    "        pat = re.compile(regex)\n",
    "        while True:\n",
    "            m = pat.search(str)\n",
    "            if m is not None:\n",
    "                str = str[:m.start(group)] + replacement + str[m.end(group):]\n",
    "            else:\n",
    "                break\n",
    "        return str\n",
    "\n",
    "    clean_data_str = regex_replace(clean_data_str, r\"\\r\", 0, \"\")\n",
    "    clean_data_str = regex_replace(clean_data_str, r\"\\S(\\n)\\S\", 1, \" \")\n",
    "    clean_data_str = regex_replace(clean_data_str, r\"\\u201C\", 0, '\"')\n",
    "    clean_data_str = regex_replace(clean_data_str, r\"\\u201D\", 0, '\"')\n",
    "    clean_data_str = regex_replace(clean_data_str, r\"_\", 0, '')\n",
    "    clean_data_str = clean_data_str[1434:-18595]\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(clean_data_str)\n",
    "        \n",
    "path = pathlib.Path(\"alice.txt\")\n",
    "\n",
    "if not path.exists():\n",
    "    create_alice_txt(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tWYdCgONMXKm",
    "outputId": "2f1f6c44-e2c9-4f92-9bdc-d810d2b3bc35",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#-----------------------#\n",
    "#  Training Parameters  #\n",
    "#-----------------------#\n",
    "\n",
    "# Use a string to set the initial value of the soft prompt.\n",
    "# Be aware of the number of tokens.\n",
    "initial_prompt = \"A surreal children's fantasy story set in a subterranean world populated by peculiar anthropomorphic creatures.\\n\"\n",
    "\n",
    "print(f\"Initial prompt length: {len(tokenizer.encode(initial_prompt))} tokens\")\n",
    "\n",
    "# Decide the length of your training blocks in tokens.\n",
    "# Safe sizes for gpt-neo-2.7B-halved:\n",
    "#  - 700 on a Colab T4 (16GB)\n",
    "#  - 400 on a Colab K80 (12GB)\n",
    "#  - 32 on a GTX1080 (8GB)\n",
    "# If it seems a bit small, don't worry!\n",
    "# Soft prompts can be moved forward in context for the best effect.\n",
    "block_size = 700\n",
    "\n",
    "# Name your soft prompt project.\n",
    "sp_name = 'alice-cyclic-dropout-2'\n",
    "\n",
    "# What's the name of model you'll be using?\n",
    "# e.g. gpt2, gpt2-large, gpt-neo-2.7B\n",
    "# (This will be added to the project directory and soft prompt name)\n",
    "# model_name = 'dolly-v2-3b'\n",
    "\n",
    "# Specify the model directory or huggingface name.\n",
    "# model_dir = 'databricks/dolly-v2-3b'\n",
    "model_base_name = model_name.split(\"/\")[-1]\n",
    "\n",
    "# The above model_dir will download GPT2 1.5B from Huggingface as a baseline.\n",
    "# It is recommended to use finetuneanon's FP16 fork of gpt-neo-2.7B, which can be downloaded from this magnet link:\n",
    "# magnet:?xt=urn:btih:f50bb4e259d2f96aa9151443950b0d2b899a097c&dn=gpt-neo-2.7B-halved&tr=http%3A%2F%2Fopenbittorrent.com%3A80%2Fannounce&tr=http%3A%2F%2Ft.nyaatracker.com%3A80%2Fannounce&tr=udp%3A%2F%2Fopen.stealth.si%3A80%2Fannounce\n",
    "# Once you've saved it to your local machine, create a 'models' folder in your Google Drive and upload it there,\n",
    "# then uncomment the following:\n",
    "#model_dir = \"/content/drive/MyDrive/models/gpt-neo-2.7B-halved/\"\n",
    "\n",
    "# Should be 'gpt2' or 'gpt-neo'.\n",
    "model_type = 'gpt-neox'\n",
    "\n",
    "# Specify the path to the text file used for training.\n",
    "text_path = \"alice.txt\"\n",
    "# You can also use something uploaded to your Google Drive, e.g.\n",
    "#text_path = \"/content/drive/MyDrive/datasets/nm_burning_chrome.txt\"\n",
    "\n",
    "# Specify the project directory.\n",
    "project_dir = f\"/home/simon/.mkultra/soft_prompts/{sp_name}-{model_base_name}/\"\n",
    "\n",
    "# Checkpoint interval in steps.\n",
    "checkpoint_interval = 20\n",
    "\n",
    "# Evaluation interval in steps.\n",
    "eval_interval = 5\n",
    "\n",
    "# How many blocks to use for evaluation.\n",
    "eval_blocks = 16\n",
    "\n",
    "# Adafactor hyperparameters\n",
    "optimizer_params = {\n",
    "    # Fixed learning rate, recommend 1e-4 to 1e-3\n",
    "    \"lr\": 2e-4,\n",
    "    \n",
    "    # 1st momentum, recommend 0\n",
    "    \"beta1\": 0.0,\n",
    "\n",
    "    # 2nd momentum decay schedule, recommend -0.3 (lower is slower)\n",
    "    \"decay_rate\": -0.8,\n",
    "\n",
    "    # Weight decay, recommend 1e-5\n",
    "    \"weight_decay\": 0.1,\n",
    "    \n",
    "    # Update scaling, recommend False\n",
    "    \"scale_parameter\": False,\n",
    "    \n",
    "    # Built-in LR scheduler, recommend False\n",
    "    \"relative_step\": False\n",
    "    }\n",
    "\n",
    "# LR scheduler parameters\n",
    "scheduler_params = {\n",
    "    \"num_warmup_steps\": 10,\n",
    "    \"num_cycles\": 8,\n",
    "    \"num_training_steps\": 1000\n",
    "}\n",
    "\n",
    "# (Use these for GPT-Neo)\n",
    "#scheduler_params = {\n",
    "#    \"num_warmup_steps\": 10,\n",
    "#    \"num_cycles\": 4,\n",
    "#    \"num_training_steps\": 240\n",
    "#}\n",
    "\n",
    "base_acc_steps = 16\n",
    "acc_doubling_rate = 0\n",
    "plateau_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "BZAT90ZCMXKo",
    "outputId": "455d3100-04f5-49db-c96d-3e3b95ddf270",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Initialize project\n",
    "#@markdown This will load the latest checkpoint if the project directory already exists.\n",
    "\n",
    "from refuge._vendor.mkultra.soft_prompt import SoftPrompt\n",
    "from transformers import Adafactor\n",
    "import os\n",
    "\n",
    "filename_for_checkpoint = lambda step: f\"{sp_name}-{model_base_name}-step-{step}.json\"\n",
    "loaded_sp = None\n",
    "project_files = None\n",
    "\n",
    "# Look for existing project directory\n",
    "try:\n",
    "    os.makedirs(project_dir)\n",
    "    print(f\"Created project directory at {project_dir}\")\n",
    "except FileExistsError:\n",
    "    print(f\"Found project directory at {project_dir}\")\n",
    "\n",
    "# Look for existing checkpoints\n",
    "project_files = os.listdir(project_dir)\n",
    "if project_files is not None:\n",
    "    checkpoint_files = [check_file for check_file in project_files if ('-step-' in check_file) ]\n",
    "\n",
    "    if len(checkpoint_files) > 0:\n",
    "        highest_step = max([ int(check_file[check_file.rfind('-step-')+6:-5]) for check_file in checkpoint_files ])\n",
    "        loaded_sp = SoftPrompt.from_file( os.path.join(project_dir, filename_for_checkpoint(highest_step)) )\n",
    "        print(f\"Loading latest checkpoint: {highest_step}\")\n",
    "    else:\n",
    "        print(\"No checkpoints found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "pf7wZnxNtR29",
    "outputId": "0f1eb6bf-edce-43cb-9201-a5e995ea8879",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Process dataset\n",
    "#@markdown This will load an existing set\n",
    "#@markdown of tokens if present in the project directory.\n",
    "\n",
    "import json\n",
    "import math\n",
    "\n",
    "text_tokenized = None\n",
    "tokens_path = os.path.join(project_dir,\"tokens.json\")\n",
    "\n",
    "# See if we already have a tokens file\n",
    "try:\n",
    "    with open(tokens_path, 'r', encoding='utf-8') as file:\n",
    "        text_tokenized = json.load(file)\n",
    "        print(\"Loaded existing tokens.json file\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"No tokens.json exists, creating it...\")\n",
    "\n",
    "# If not, make one now\n",
    "if text_tokenized is None:\n",
    "\n",
    "    with open(text_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    text_tokenized = tokenizer.encode(text)\n",
    "    \n",
    "    with open(tokens_path, 'x', encoding='utf-8') as file:\n",
    "        json.dump(text_tokenized, file)\n",
    "\n",
    "text_length = len(text_tokenized)\n",
    "num_blocks = math.ceil(text_length/block_size)\n",
    "\n",
    "print(f\"Length of text: {len(text_tokenized)} tokens\")\n",
    "print(f\"Number of blocks: {num_blocks}, each {block_size} tokens\")\n",
    "\n",
    "# Partition tokens into blocks\n",
    "blocks = list()\n",
    "for block_num in range(num_blocks):\n",
    "    start = block_num * block_size\n",
    "    end = min(start + block_size, text_length)\n",
    "    blocks.append( text_tokenized[start:end] )\n",
    "\n",
    "block_order_path = os.path.join(project_dir, \"block_order.json\")\n",
    "\n",
    "# See if we already have a block_order file\n",
    "try:\n",
    "    with open(block_order_path, 'r', encoding='utf-8') as file:\n",
    "        block_order = json.load(file)\n",
    "        print(\"Loaded existing block_order.json file\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"No block_order.json exists, creating it...\")\n",
    "    block_order = [*range(num_blocks)]\n",
    "\n",
    "    with open(block_order_path, 'x', encoding='utf-8') as file:\n",
    "        json.dump(block_order, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TI_f3H5lXeLt",
    "outputId": "e03e52db-a742-4020-e671-129a27b00b1d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Initialize soft prompt in model\n",
    "#@markdown If a checkpoint is present, use that.\n",
    "if loaded_sp is None:\n",
    "    initial_sp = SoftPrompt.from_string(initial_prompt, model, tokenizer)\n",
    "    print(f\"Initial prompt length: {len(initial_sp)}\")\n",
    "    model.set_soft_prompt(initial_sp)\n",
    "\n",
    "    sp_step = 0\n",
    "    eval_loss = 100\n",
    "else:\n",
    "    model.set_soft_prompt(loaded_sp)\n",
    "    sp_step = loaded_sp._metadata['step']\n",
    "    eval_loss = loaded_sp._metadata['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "nYaLKE1YtR3C",
    "outputId": "dea1b23d-285b-4f5f-d242-834e63a03e3e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure number of steps to train for.\n",
    "# One step is (acc_steps) forward passes.\n",
    "num_training_steps = scheduler_params['num_training_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "LlqN3HcGQMZ0",
    "outputId": "aeffd993-9b58-491a-8311-1e6db863ac21",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, Adafactor\n",
    "import transformers\n",
    "\n",
    "# Feed soft params to optimizer\n",
    "optimizer_params['params'] = [model.get_soft_params()]\n",
    "optimizer = Adafactor(**optimizer_params)\n",
    "optimizer.state['step'] = sp_step\n",
    "\n",
    "scheduler_params['optimizer'] = optimizer\n",
    "scheduler = transformers.get_cosine_with_hard_restarts_schedule_with_warmup(**scheduler_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "563f05f9831a4913a9fb9fb7116374c4",
      "367016fea14b4eabaf0a1a1c424e63a4",
      "57bcb1b7ab324955ac48b7e22c51d695",
      "034feddd372b458d8e82848fc4db02fc",
      "870cc0cf45cb4895b400ac2c0d1df2d4",
      "7668769f21704c5b9903086b69c15029",
      "16ff7090ebcd4388b1a747060b99a327",
      "9396770b5f4842ac8e01c56dc5b80909"
     ]
    },
    "id": "ImdPj_CftR3C",
    "outputId": "89ee86d1-4273-420a-bce4-37201109d3c0",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#@title Train the soft prompt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "loss_log_path = os.path.join(project_dir,\"loss_log.csv\")\n",
    "bar = tqdm(total=num_training_steps)\n",
    "optimizer.state['step'] = sp_step\n",
    "evals_since_last_improvement = 0\n",
    "best_eval = float('inf')\n",
    "\n",
    "# Fix eval order\n",
    "eval_order = [*range(num_blocks)]\n",
    "# random.seed(1234)\n",
    "random.shuffle(eval_order)\n",
    "\n",
    "# Function for gradient accumulation scheduling\n",
    "def get_acc_steps(sp_step):\n",
    "    if acc_doubling_rate != 0:\n",
    "        return round(base_acc_steps * math.pow(2, (sp_step / acc_doubling_rate)))\n",
    "    else:\n",
    "        return base_acc_steps\n",
    "\n",
    "for session_step in range(num_training_steps):\n",
    "      model.train()\n",
    "\n",
    "      acc_steps = get_acc_steps(sp_step)\n",
    "\n",
    "      for i in range(acc_steps):\n",
    "          idx = (sp_step*acc_steps + i) % num_blocks\n",
    "\n",
    "          # Shuffle blocks every epoch\n",
    "          if idx == 0:\n",
    "              random.shuffle(block_order)\n",
    "              with open(block_order_path, 'w', encoding='utf-8') as file:\n",
    "                  json.dump(block_order, file)\n",
    "\n",
    "          block = blocks[block_order[idx]]\n",
    "\n",
    "          input_ids = torch.LongTensor(block).unsqueeze(0).cuda().detach()\n",
    "          \n",
    "          # Forward pass and optimize\n",
    "          outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "          loss = outputs.loss\n",
    "          loss.backward()\n",
    "\n",
    "          instant_loss = loss.item()\n",
    "          if math.isnan(instant_loss):\n",
    "              torch.cuda.empty_cache()\n",
    "              raise KeyboardInterrupt\n",
    "\n",
    "          # Discard tensor that was moved to GPU\n",
    "          del input_ids\n",
    "          torch.cuda.empty_cache()\n",
    "\n",
    "      # Accumulate gradients\n",
    "      optimizer.step()\n",
    "      lr = optimizer.param_groups[0][\"lr\"]\n",
    "      scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      if math.isnan(instant_loss):\n",
    "          torch.cuda.empty_cache()\n",
    "          raise KeyboardInterrupt\n",
    "\n",
    "      # Evaluate model and plot loss\n",
    "      if sp_step%eval_interval == 0:\n",
    "          model.eval()\n",
    "          torch.cuda.empty_cache()\n",
    "          eval_loss = 0\n",
    "\n",
    "          with torch.no_grad():\n",
    "              for eval_step in range(eval_blocks):\n",
    "                  block = blocks[eval_order[eval_step]]\n",
    "                  input_ids = torch.LongTensor(block).unsqueeze(0).cuda().detach()\n",
    "                  eval_loss += model(input_ids=input_ids, labels=input_ids).loss.item()\n",
    "                  \n",
    "                  # Discard tensor that was moved to GPU\n",
    "                  del input_ids\n",
    "                  torch.cuda.empty_cache()\n",
    "\n",
    "          eval_loss /= eval_blocks\n",
    "\n",
    "          with open(loss_log_path, 'a', encoding='utf-8') as file:\n",
    "              file.write(f\"{sp_step},{eval_loss}\\n\")\n",
    "          \n",
    "          # Stop if loss has plateaued\n",
    "          if plateau_steps != 0:\n",
    "              if eval_loss < best_eval:\n",
    "                  best_eval = eval_loss\n",
    "                  evals_since_last_improvement = 0\n",
    "              else:\n",
    "                  evals_since_last_improvement += 1\n",
    "              if evals_since_last_improvement > plateau_steps:\n",
    "                  print(f\"No improvement for {plateau_steps} evals\")\n",
    "                  break\n",
    "\n",
    "      # Save checkpoint every so often\n",
    "      if sp_step%checkpoint_interval == 0:\n",
    "          sp = SoftPrompt.from_tuning_model(model,\n",
    "              {\"name\" : sp_name + f\"-step-{sp_step}\",\n",
    "               \"step\"  : sp_step,\n",
    "               \"loss\"  : eval_loss})\n",
    "          sp.to_file( os.path.join( project_dir,filename_for_checkpoint(sp_step) ) )\n",
    "\n",
    "      bar.set_postfix({\n",
    "          \"Model Step\" : sp_step,\n",
    "          \"Eval Loss\"  : \"{el:.5f}\".format(el=eval_loss),\n",
    "          \"Acc Steps\"  : acc_steps,\n",
    "          \"LR\"         : lr\n",
    "      })\n",
    "      bar.update(1)\n",
    "      sp_step += 1\n",
    "\n",
    "# Save a checkpoint once done\n",
    "sp = SoftPrompt.from_tuning_model(model,\n",
    "    {\"name\"  : sp_name + f\"-step-{sp_step}\",\n",
    "     \"step\"  : sp_step,\n",
    "     \"loss\"  : eval_loss})\n",
    "sp.to_file( os.path.join( project_dir,filename_for_checkpoint(sp_step) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "-UdO6kRiaptn",
    "outputId": "d06311f9-23b6-4f74-83cc-4e899a9f9579",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Flush memory after interrupting training\n",
    "#@markdown This will *hopefully* prevent a CUDA out-of-memory error.\n",
    "try:\n",
    "  del input_ids\n",
    "except Exception:\n",
    "  pass\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "JPZpBzbvaoru",
    "outputId": "5c8fc5d2-ee82-4c94-c2a7-2d1e9eb52f5f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import numpy as np\n",
    "\n",
    "fname2 = cbook.get_sample_data(loss_log_path, asfileobj=False)\n",
    "with cbook.get_sample_data(loss_log_path) as file:\n",
    "    array = np.loadtxt(file, delimiter=\",\")\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(array[:, 0], array[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "YKypaYDOtR3E",
    "outputId": "43002925-1b61-4775-9bbb-1415bfc72c46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try generating with your model\n",
    "model.eval()\n",
    "\n",
    "# Restore soft prompt from checkpoint\n",
    "# (Use above graph to find a good stopping point and check project directory for valid checkpoints)\n",
    "sp = SoftPrompt.from_file( os.path.join(project_dir, filename_for_checkpoint(400)) )\n",
    "model.set_soft_prompt(sp)\n",
    "\n",
    "test = \"Alice sipped her tea as the white rabbit gloated about his vast collection of pocket watches\"\n",
    "\n",
    "call = tokenizer(test, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "basic_output = model.generate(\n",
    "    input_ids=call,\n",
    "    do_sample=True,\n",
    "    min_length=call.shape[-1] + 200,\n",
    "    max_length=call.shape[-1] + 200,\n",
    "    temperature=1.0,\n",
    "    # tfs = 0.9,\n",
    "    repetition_penalty = 3.0,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "4RnpmIbbjUTI",
    "outputId": "93d9b3d1-636c-413e-e86b-c20a1b3afe8d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Purge soft prompt for comparison.\n",
    "model.initialize_soft_prompt(n_tokens=1)\n",
    "\n",
    "test = \"Alice sipped her tea as the white rabbit gloated about his vast collection of pocket watches\"\n",
    "\n",
    "call = tokenizer(test, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "basic_output = model.generate(\n",
    "    input_ids=call,\n",
    "    do_sample=True,\n",
    "    min_length=call.shape[-1] + 200,\n",
    "    max_length=call.shape[-1] + 200,\n",
    "    temperature=1.0,\n",
    "    # tfs = 0.9,\n",
    "    repetition_penalty = 3.0,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(basic_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tuning_finetune_alice.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
  },
  "kernelspec": {
   "display_name": "refuge",
   "language": "python",
   "name": "refuge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
